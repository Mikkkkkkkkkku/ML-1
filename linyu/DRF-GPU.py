import time
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, RegressorMixin
import warnings
import joblib

warnings.filterwarnings('ignore')


class QuantileNet(nn.Module):
    """åˆ†ä½æ•°å›å½’ç¥ç»ç½‘ç»œ"""

    def __init__(self, input_size, output_size, hidden_sizes=[128, 64, 32]):
        super(QuantileNet, self).__init__()
        layers = []
        prev_size = input_size

        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.1))
            prev_size = hidden_size

        layers.append(nn.Linear(prev_size, output_size))
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)


class DistributionalRandomForestGPU(BaseEstimator, RegressorMixin):
    """
    Distributional Random Forests - PyTorch GPUç‰ˆæœ¬
    ä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿåˆ†ä½æ•°å›å½’æ£®æ—
    """

    def __init__(self, quantiles=[0.05, 0.25, 0.5, 0.75, 0.95],
                 hidden_sizes=[128, 64, 32], batch_size=256,
                 n_epochs=100, learning_rate=0.001, random_state=42):
        self.quantiles = quantiles
        self.hidden_sizes = hidden_sizes
        self.batch_size = batch_size
        self.n_epochs = n_epochs
        self.learning_rate = learning_rate
        self.random_state = random_state
        self.models = {}
        self.scalers = {}
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

    def quantile_loss(self, y_pred, y_true, q):
        """åˆ†ä½æ•°æŸå¤±å‡½æ•°"""
        errors = y_true - y_pred
        return torch.max((q - 1) * errors, q * errors).mean()

    def fit_single_quantile(self, X, y, quantile):
        """è®­ç»ƒå•ä¸ªåˆ†ä½æ•°æ¨¡å‹"""
        torch.manual_seed(self.random_state)

        # æ•°æ®å‡†å¤‡
        X_tensor = torch.FloatTensor(X).to(self.device)
        y_tensor = torch.FloatTensor(y).to(self.device)

        dataset = TensorDataset(X_tensor, y_tensor)
        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)

        # æ¨¡å‹åˆå§‹åŒ–
        input_size = X.shape[1]
        model = QuantileNet(input_size, 1, self.hidden_sizes).to(self.device)
        optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)

        # è®­ç»ƒå¾ªç¯
        model.train()
        for epoch in range(self.n_epochs):
            total_loss = 0
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                predictions = model(batch_X).squeeze()
                loss = self.quantile_loss(predictions, batch_y, quantile)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()

            if (epoch + 1) % 20 == 0:
                print(
                    f"    åˆ†ä½æ•° {quantile} - è½®æ¬¡ {epoch + 1}/{self.n_epochs}, æŸå¤±: {total_loss / len(dataloader):.4f}")

        return model

    def fit(self, X, y):
        """è®­ç»ƒæ‰€æœ‰åˆ†ä½æ•°æ¨¡å‹"""
        print(f"è®­ç»ƒPyTorch GPUåˆ†ä½æ•°æ¨¡å‹ ({len(self.quantiles)}ä¸ªåˆ†ä½æ•°)...")

        # ç¡®ä¿æ•°æ®æ˜¯numpyæ•°ç»„
        if hasattr(X, 'values'):
            X = X.values
        if hasattr(y, 'values'):
            y = y.values

        X = X.astype(np.float32)
        y = y.astype(np.float32)

        # ä¸ºæ¯ä¸ªåˆ†ä½æ•°è®­ç»ƒæ¨¡å‹
        for i, q in enumerate(self.quantiles):
            print(f"  è®­ç»ƒåˆ†ä½æ•° {q} ({i + 1}/{len(self.quantiles)})...")
            model = self.fit_single_quantile(X, y, q)
            self.models[q] = model

        return self

    def predict(self, X, return_distribution=False):
        """é¢„æµ‹"""
        if hasattr(X, 'values'):
            X = X.values
        X = X.astype(np.float32)
        X_tensor = torch.FloatTensor(X).to(self.device)

        if return_distribution:
            # è¿”å›å®Œæ•´åˆ†å¸ƒ
            distribution = {}
            for q, model in self.models.items():
                model.eval()
                with torch.no_grad():
                    pred = model(X_tensor).cpu().numpy().flatten()
                distribution[q] = pred
            return distribution
        else:
            # è¿”å›ä¸­ä½æ•°
            model = self.models[0.5]
            model.eval()
            with torch.no_grad():
                pred = model(X_tensor).cpu().numpy().flatten()
            return pred

    def predict_interval(self, X, confidence=0.9):
        """é¢„æµ‹åŒºé—´"""
        alpha = (1 - confidence) / 2
        lower_q = alpha
        upper_q = 1 - alpha

        # æ‰¾åˆ°æœ€æ¥è¿‘çš„åˆ†ä½æ•°
        lower_quantile = min(self.quantiles, key=lambda x: abs(x - lower_q))
        upper_quantile = min(self.quantiles, key=lambda x: abs(x - upper_q))

        distribution = self.predict(X, return_distribution=True)
        lower_bound = distribution[lower_quantile]
        upper_bound = distribution[upper_quantile]

        return lower_bound, upper_bound

    def get_uncertainty(self, X):
        """è·å–é¢„æµ‹ä¸ç¡®å®šæ€§"""
        distribution = self.predict(X, return_distribution=True)
        predictions = np.array(list(distribution.values()))
        uncertainty = np.percentile(predictions, 75, axis=0) - np.percentile(predictions, 25, axis=0)
        return uncertainty


# ä¸»ç¨‹åºå¼€å§‹
start_time = time.time()

# æ£€æŸ¥GPU
print("æ£€æŸ¥GPUå¯ç”¨æ€§...")
if torch.cuda.is_available():
    print(f"âœ… GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
    print(f"âœ… GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB")
else:
    print("âŒ GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")

# åŠ è½½æ•°æ®
print("åŠ è½½æ•°æ®...")
train_dataSet = pd.read_csv('modified_æ•°æ®é›†Time_Series662_detail.dat')
test_dataSet = pd.read_csv('modified_æ•°æ®é›†Time_Series661_detail.dat')

columns = ['T_SONIC', 'CO2_density', 'CO2_density_fast_tmpr', 'H2O_density', 'H2O_sig_strgth', 'CO2_sig_strgth']
noise_columns = ['Error_T_SONIC', 'Error_CO2_density', 'Error_CO2_density_fast_tmpr', 'Error_H2O_density',
                 'Error_H2O_sig_strgth', 'Error_CO2_sig_strgth']


def fast_feature_engineering(data):
    """æé€Ÿç‰¹å¾å·¥ç¨‹"""
    features = data[noise_columns].copy()
    features['noise_mean'] = features[noise_columns].mean(axis=1)
    features = features.fillna(method='bfill')
    return features


print("å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
X_train_enhanced = fast_feature_engineering(train_dataSet)
X_test_enhanced = fast_feature_engineering(test_dataSet)

y_train = train_dataSet[columns].values
y_test = test_dataSet[columns].values

print(f"æ•°æ®å½¢çŠ¶ - è®­ç»ƒé›†: {X_train_enhanced.shape}, æµ‹è¯•é›†: {X_test_enhanced.shape}")


def sample_training_data(X, y, sample_ratio=0.6):
    """é‡‡æ ·è®­ç»ƒæ•°æ®"""
    n_samples = int(len(X) * sample_ratio)
    indices = np.random.choice(len(X), n_samples, replace=False)
    return X.iloc[indices] if hasattr(X, 'iloc') else X[indices], y[indices]


print("è®­ç»ƒæ•°æ®é‡‡æ ·ä¸­...")
X_train_sampled, y_train_sampled = sample_training_data(X_train_enhanced, y_train, 0.6)
print(f"é‡‡æ ·åå½¢çŠ¶ - è®­ç»ƒé›†: {X_train_sampled.shape}")

print("ç›®æ ‡å˜é‡æ ‡å‡†åŒ–...")
scaler_y = StandardScaler()
y_train_scaled = scaler_y.fit_transform(y_train_sampled)

print("å¼€å§‹PyTorch GPUåˆ†ä½æ•°æ¨¡å‹è®­ç»ƒ...")
models = []
cv_scores = []

# GPUä¼˜åŒ–å‚æ•°
gpu_params = {
    'quantiles': [0.05, 0.25, 0.5, 0.75, 0.95],
    'hidden_sizes': [256, 128, 64],  # æ›´å¤§çš„ç½‘ç»œ
    'batch_size': 512,  # æ›´å¤§çš„æ‰¹æ¬¡
    'n_epochs': 100,
    'learning_rate': 0.001,
    'random_state': 42
}

for i, col in enumerate(columns):
    print(f"\nè®­ç»ƒ {col} ({i + 1}/{len(columns)})...")

    # ä½¿ç”¨PyTorch GPUæ¨¡å‹
    model = DistributionalRandomForestGPU(**gpu_params)

    # è®­ç»ƒæ¨¡å‹
    model.fit(X_train_sampled.values, y_train_scaled[:, i])
    models.append(model)

    # å¿«é€ŸéªŒè¯
    train_pred = model.predict(X_train_sampled.values)
    train_mae = np.mean(np.abs(y_train_scaled[:, i] - train_pred))
    cv_scores.append(train_mae)

    print(f"  {col} è®­ç»ƒMAE: {train_mae:.4f}")

print("\nè¿›è¡Œå®Œæ•´æµ‹è¯•é›†é¢„æµ‹...")
test_predictions_scaled = []
test_uncertainties = []

for i, col in enumerate(columns):
    print(f"é¢„æµ‹ {col}...")

    # é¢„æµ‹
    pred_scaled = models[i].predict(X_test_enhanced.values)
    test_predictions_scaled.append(pred_scaled)

    # è·å–ä¸ç¡®å®šæ€§
    uncertainty = models[i].get_uncertainty(X_test_enhanced.values)
    test_uncertainties.append(uncertainty)

# åæ ‡å‡†åŒ–
y_pred_scaled = np.column_stack(test_predictions_scaled)
y_pred = scaler_y.inverse_transform(y_pred_scaled)

# è®¡ç®—ä¸ç¡®å®šæ€§ï¼ˆåŸå§‹å°ºåº¦ï¼‰
uncertainties_original = []
for i in range(len(columns)):
    col_scale = scaler_y.scale_[i]
    uncertainty_original = test_uncertainties[i] * col_scale
    uncertainties_original.append(uncertainty_original)

uncertainties_original = np.column_stack(uncertainties_original)

# è®¡ç®—è¯¯å·®
test_mae = np.mean(np.abs(y_test - y_pred), axis=0)
final_test_error = test_mae.mean()

# ä¿å­˜ç»“æœ
print(f"\nä¿å­˜æ‰€æœ‰ {len(y_test)} æ¡ç»“æœ...")
results = []
batch_size = 10000
total_batches = (len(y_test) + batch_size - 1) // batch_size

for batch_idx in range(total_batches):
    start_idx = batch_idx * batch_size
    end_idx = min((batch_idx + 1) * batch_size, len(y_test))

    batch_results = []
    for j in range(start_idx, end_idx):
        True_Value = y_test[j]
        Predicted_Value = y_pred[j]
        error = np.abs(True_Value - Predicted_Value)
        uncertainty = uncertainties_original[j]

        formatted_true_value = ' '.join([f"{val:.6f}" for val in True_Value])
        formatted_predicted_value = ' '.join([f"{val:.6f}" for val in Predicted_Value])
        formatted_error = ' '.join([f"{val:.6f}" for val in error])
        formatted_uncertainty = ' '.join([f"{val:.6f}" for val in uncertainty])

        batch_results.append([formatted_true_value, formatted_predicted_value, formatted_error, formatted_uncertainty])

    result_df = pd.DataFrame(batch_results, columns=['True_Value', 'Predicted_Value', 'Error', 'Uncertainty'])
    if batch_idx == 0:
        result_df.to_csv("result_DRF_PyTorch_GPU.csv", index=False)
    else:
        result_df.to_csv("result_DRF_PyTorch_GPU.csv", mode='a', header=False, index=False)

    print(f"  è¿›åº¦: {end_idx}/{len(y_test)} ({end_idx / len(y_test) * 100:.1f}%)")

end_time = time.time()
total_time = end_time - start_time

print(f"\næœ€ç»ˆæµ‹è¯•è¯¯å·®: {final_test_error:.6f}")
print(f"æ€»è€—æ—¶: {total_time / 60:.2f} åˆ†é’Ÿ")

print("\n" + "=" * 50)
print("PyTorch GPUç‰ˆæœ¬ç‰¹æ€§")
print("=" * 50)
print("âœ“ ä½¿ç”¨PyTorch GPUåŠ é€Ÿ")
print("âœ“ ç¥ç»ç½‘ç»œåˆ†ä½æ•°å›å½’")
print("âœ“ å®Œæ•´çš„åˆ†å¸ƒé¢„æµ‹")
print("âœ“ é‡åŒ–ä¸ç¡®å®šæ€§")
print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {models[0].device}")

if final_test_error < 0.5:
    print("ğŸ‰ æˆåŠŸï¼æ¨¡å‹å¹³å‡è¯¯å·®ä½äºç›®æ ‡å€¼ 0.5ï¼")
else:
    print(f"ğŸ“Š å½“å‰è¯¯å·® {final_test_error:.6f}")

# æ¸…ç†GPUå†…å­˜
if torch.cuda.is_available():
    torch.cuda.empty_cache()
import os
import gc  # ä¿®å¤ï¼šå¯¼å…¥gcæ¨¡å—

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import time
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
import warnings
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
import joblib

warnings.filterwarnings('ignore')


# GPUé…ç½®
def setup_gpu():
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            return True
        except:
            return False
    return False


use_gpu = setup_gpu()
tf.random.set_seed(42)
np.random.seed(42)

BATCH_SIZE = 4096 if use_gpu else 1024  # ä¼˜åŒ–ï¼šå¢å¤§æ‰¹æ¬¡ï¼ˆåŠ é€Ÿè®­ç»ƒï¼Œæå‡GPUåˆ©ç”¨ç‡ï¼‰
EPOCHS = 200  # ä¼˜åŒ–ï¼šå¢åŠ MLPè®­ç»ƒè½®æ•°ï¼ˆé’ˆå¯¹å¤æ‚å˜é‡ï¼‰


class RobustMLP:
    """ç¨³å¥çš„MLPæ¨¡å‹ï¼ˆä¼˜åŒ–å‚æ•°ï¼‰"""

    def __init__(self, hidden_layers=[128, 64, 32], learning_rate=0.001,
                 batch_size=2048, epochs=100, l2_reg=0.0001, dropout_rate=0.1):
        self.hidden_layers = hidden_layers
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.epochs = epochs
        self.l2_reg = l2_reg
        self.dropout_rate = dropout_rate
        self.model = None
        self.scaler_X = None
        self.scaler_y = None

    def _create_model(self, input_dim, output_dim):
        model = Sequential()

        model.add(Dense(self.hidden_layers[0], activation='relu',
                        kernel_regularizer=l2(self.l2_reg),
                        input_shape=(input_dim,)))
        model.add(BatchNormalization())
        model.add(Dropout(self.dropout_rate))

        for units in self.hidden_layers[1:]:
            model.add(Dense(units, activation='relu',
                            kernel_regularizer=l2(self.l2_reg)))
            model.add(BatchNormalization())
            model.add(Dropout(self.dropout_rate))

        model.add(Dense(output_dim, activation='linear'))

        model.compile(
            optimizer=Adam(learning_rate=self.learning_rate),
            loss='mse',
            metrics=['mae']
        )

        return model

    def fit(self, X, y):
        print("è®­ç»ƒç¨³å¥MLPæ¨¡å‹...")

        self.scaler_X = StandardScaler()
        X_scaled = self.scaler_X.fit_transform(X)

        self.scaler_y = StandardScaler()
        y_scaled = self.scaler_y.fit_transform(y)

        self.model = self._create_model(X_scaled.shape[1], y_scaled.shape[1])

        callbacks = [
            EarlyStopping(monitor='val_mae', patience=15, restore_best_weights=True, verbose=1),  # ä¼˜åŒ–ï¼šç›‘æ§MAEï¼ˆæ›´è´´åˆè¯„ä»·æŒ‡æ ‡ï¼‰
            ReduceLROnPlateau(monitor='val_mae', factor=0.5, patience=8, min_lr=1e-7, verbose=1)  # ä¼˜åŒ–ï¼šè°ƒæ•´å­¦ä¹ ç‡è¡°å‡å‚æ•°
        ]

        history = self.model.fit(
            X_scaled, y_scaled,
            batch_size=self.batch_size,
            epochs=self.epochs,
            callbacks=callbacks,
            verbose=1,
            validation_split=0.2,  # ä¼˜åŒ–ï¼šå¢å¤§éªŒè¯é›†æ¯”ä¾‹ï¼ˆæ›´å¥½ç›‘æ§è¿‡æ‹Ÿåˆï¼‰
            shuffle=True
        )

        best_val_mae = min(history.history['val_mae'])
        print(f"è®­ç»ƒå®Œæˆ, æœ€ä½³val_mae: {best_val_mae:.4f}")  # ä¼˜åŒ–ï¼šè¾“å‡ºéªŒè¯é›†MAEï¼ˆæ›´ç›´è§‚ï¼‰

        return self

    def predict(self, X):
        X_scaled = self.scaler_X.transform(X)
        y_pred_scaled = self.model.predict(X_scaled, verbose=0, batch_size=self.batch_size)
        return self.scaler_y.inverse_transform(y_pred_scaled)


class SafeHybridModel:
    """å®‰å…¨çš„æ··åˆæ¨¡å‹ - ä¿®å¤+ä¼˜åŒ–"""

    def __init__(self, target_columns):
        self.target_columns = target_columns
        self.models = []
        self.scaler_X = None
        self.scaler_y = None
        self.selected_features_mask = None
        self.y_mean = None
        self.y_std = None

    def fit(self, X, y):
        print("è®­ç»ƒå®‰å…¨æ··åˆæ¨¡å‹...")

        if hasattr(X, 'values'):
            X_data = X.values
        else:
            X_data = X

        X_processed, self.selected_features_mask = self._preprocess_features(X_data, is_training=True)

        self.scaler_X = StandardScaler()
        X_scaled = self.scaler_X.fit_transform(X_processed)

        self.scaler_y = StandardScaler()
        y_scaled = self.scaler_y.fit_transform(y)

        self.y_mean = np.mean(y, axis=0)
        self.y_std = np.std(y, axis=0)

        self.models = []

        for i in range(y_scaled.shape[1]):
            print(f"  ç›®æ ‡å˜é‡ {i + 1}/{y_scaled.shape[1]} ({self.target_columns[i]})...")

            # ä¼˜åŒ–ï¼šCO2ç›¸å…³å˜é‡ï¼ˆi=1,2ï¼‰å¢å¼ºéšæœºæ£®æ—å¤æ‚åº¦
            if i in [1, 2]:  # CO2ç›¸å…³å˜é‡ï¼ˆéšæœºæ£®æ—ï¼‰
                print("    ä½¿ç”¨éšæœºæ£®æ—...")
                model = RandomForestRegressor(
                    n_estimators=100,  # ä¼˜åŒ–ï¼šå¢åŠ æ ‘æ•°é‡ï¼ˆæå‡æ‹Ÿåˆèƒ½åŠ›ï¼‰
                    max_depth=20,  # ä¼˜åŒ–ï¼šåŠ æ·±æ ‘æ·±åº¦ï¼ˆæ•æ‰å¤æ‚å…³ç³»ï¼‰
                    min_samples_split=8,  # ä¼˜åŒ–ï¼šé™ä½åˆ†è£‚é˜ˆå€¼
                    min_samples_leaf=3,  # ä¼˜åŒ–ï¼šé™ä½å¶èŠ‚ç‚¹é˜ˆå€¼
                    random_state=42 + i,
                    n_jobs=-1,
                    verbose=0
                )
                model.fit(X_scaled, y_scaled[:, i])
            else:  # å…¶ä»–å˜é‡ï¼ˆMLPï¼‰
                print("    ä½¿ç”¨MLP...")
                # ä¼˜åŒ–ï¼šé’ˆå¯¹è¯¯å·®å¤§çš„å˜é‡è°ƒæ•´MLPç»“æ„
                if i == 0:  # T_SONICï¼ˆè¯¯å·®æœ€å¤§ï¼Œç”¨æ›´æ·±ç½‘ç»œï¼‰
                    hidden_layers = [256, 128, 64, 32]  # ä¼˜åŒ–ï¼šå¢åŠ ä¸€å±‚éšè—å±‚ï¼Œå¢å¤§ç¥ç»å…ƒæ•°
                    learning_rate = 0.0008  # ä¼˜åŒ–ï¼šé™ä½å­¦ä¹ ç‡ï¼ˆç¨³å®šè®­ç»ƒï¼‰
                    dropout_rate = 0.15  # ä¼˜åŒ–ï¼šé€‚åº¦å¢å¤§dropoutï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
                elif i == 3:  # H2O_densityï¼ˆè¯¯å·®ç¬¬äºŒå¤§ï¼‰
                    hidden_layers = [192, 96, 48]  # ä¼˜åŒ–ï¼šåŠ æ·±ç½‘ç»œ
                    learning_rate = 0.0008
                    dropout_rate = 0.15
                else:  # H2O_sig_strgthã€CO2_sig_strgthï¼ˆæ•ˆæœå¥½ï¼Œä¿æŒç»“æ„ï¼‰
                    hidden_layers = [64, 32, 16]
                    learning_rate = 0.001
                    dropout_rate = 0.1

                model = RobustMLP(
                    hidden_layers=hidden_layers,
                    learning_rate=learning_rate,
                    batch_size=BATCH_SIZE,
                    epochs=EPOCHS,
                    l2_reg=0.0005,  # ä¼˜åŒ–ï¼šå¢å¤§L2æ­£åˆ™ï¼ˆæŠ‘åˆ¶è¿‡æ‹Ÿåˆï¼‰
                    dropout_rate=dropout_rate
                )
                model.fit(X_scaled, y_scaled[:, i].reshape(-1, 1))

            self.models.append(model)

            y_pred_temp = self._predict_single_model(i, X_scaled)
            mae = mean_absolute_error(y_scaled[:, i], y_pred_temp)
            print(f"    è®­ç»ƒé›†MAE: {mae:.4f}")

        return self

    def _preprocess_features(self, X, is_training=False):
        """å®‰å…¨çš„ç‰¹å¾é¢„å¤„ç†ï¼ˆä¼˜åŒ–ï¼šæ”¾å®½ç‰¹å¾é€‰æ‹©é˜ˆå€¼ï¼‰"""
        if is_training:
            stds = np.std(X, axis=0)
            self.selected_features_mask = stds > 1e-8  # ä¼˜åŒ–ï¼šé™ä½é˜ˆå€¼ï¼ˆä¿ç•™æ›´å¤šå¼±ç‰¹å¾ï¼Œå¯èƒ½æå‡æ‹Ÿåˆï¼‰
            X_processed = X[:, self.selected_features_mask]
            print(f"ç‰¹å¾é€‰æ‹©: {X.shape[1]} -> {X_processed.shape[1]} ä¸ªç‰¹å¾")
            return X_processed, self.selected_features_mask
        else:
            if self.selected_features_mask is None:
                raise ValueError("å¿…é¡»å…ˆè®­ç»ƒæ¨¡å‹æ‰èƒ½è¿›è¡Œé¢„æµ‹")
            X_processed = X[:, self.selected_features_mask]
            return X_processed

    def _predict_single_model(self, model_idx, X_scaled):
        """é¢„æµ‹å•ä¸ªæ¨¡å‹ï¼ˆåŒºåˆ†æ¨¡å‹ç±»å‹ï¼‰"""
        model = self.models[model_idx]
        if isinstance(model, RandomForestRegressor):
            return model.predict(X_scaled)
        else:
            return model.predict(X_scaled).ravel()

    def predict(self, X):
        if hasattr(X, 'values'):
            X_data = X.values
        else:
            X_data = X

        X_processed = self._preprocess_features(X_data, is_training=False)
        X_scaled = self.scaler_X.transform(X_processed)

        predictions = []
        for i, model in enumerate(self.models):
            pred = self._predict_single_model(i, X_scaled)

            if isinstance(model, RandomForestRegressor):
                if hasattr(self.scaler_y, 'mean_') and hasattr(self.scaler_y, 'scale_'):
                    pred = pred * self.scaler_y.scale_[i] + self.scaler_y.mean_[i]
                else:
                    pred = pred * self.y_std[i] + self.y_mean[i]

            predictions.append(pred.ravel())

        return np.column_stack(predictions)


def safe_feature_engineering(data, target_columns=None, reference_columns=None):
    """å®‰å…¨çš„ç‰¹å¾å·¥ç¨‹ï¼ˆä¼˜åŒ–ï¼šå¢åŠ æ›´å¤šæœ‰æ•ˆç‰¹å¾ï¼‰"""
    features = data.copy()

    # æ’é™¤ç›®æ ‡å˜é‡ï¼Œé¿å…æ•°æ®æ³„éœ²
    if target_columns is not None:
        features = features.drop(columns=target_columns, errors='ignore')

    # ç¡®ä¿åªå¤„ç†æ•°å€¼åˆ—
    numeric_columns = features.select_dtypes(include=[np.number]).columns.tolist()

    # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    if numeric_columns:
        features['feature_mean'] = features[numeric_columns].mean(axis=1)
        features['feature_std'] = features[numeric_columns].std(axis=1)
        features['feature_max'] = features[numeric_columns].max(axis=1)  # æ–°å¢ï¼šæœ€å¤§å€¼ç‰¹å¾
        features['feature_min'] = features[numeric_columns].min(axis=1)  # æ–°å¢ï¼šæœ€å°å€¼ç‰¹å¾
        features['feature_median'] = features[numeric_columns].median(axis=1)  # æ–°å¢ï¼šä¸­ä½æ•°ç‰¹å¾

    # å™ªå£°ç›¸å…³ç‰¹å¾
    noise_columns = ['Error_T_SONIC', 'Error_CO2_density', 'Error_CO2_density_fast_tmpr',
                     'Error_H2O_density', 'Error_H2O_sig_strgth', 'Error_CO2_sig_strgth']
    available_noise_columns = [col for col in noise_columns if col in numeric_columns]

    if available_noise_columns:
        features['noise_mean'] = features[available_noise_columns].mean(axis=1)
        features['noise_std'] = features[available_noise_columns].std(axis=1)
        features['noise_max'] = features[available_noise_columns].max(axis=1)  # æ–°å¢ï¼šå™ªå£°æœ€å¤§å€¼
        features['noise_ratio'] = features['noise_mean'] / (features['feature_mean'] + 1e-8)  # æ–°å¢ï¼šå™ªå£°/ä¿¡å·æ¯”

    # å¤„ç†ç¼ºå¤±å€¼ï¼ˆä¼˜åŒ–ï¼šæ—¶é—´åºåˆ—ç”¨çº¿æ€§æ’å€¼ï¼Œæ›´åˆç†ï¼‰
    features = features.interpolate(method='linear').fillna(method='ffill').fillna(method='bfill').fillna(0)

    # ç§»é™¤æ— é™å¤§çš„å€¼ï¼ˆç”¨ä¸­ä½æ•°å¡«å……ï¼‰
    median_val = features.median().iloc[0] if not features.empty else 0
    features = features.replace([np.inf, -np.inf], median_val)

    # ç¡®ä¿ç‰¹å¾é¡ºåºä¸€è‡´
    if reference_columns is not None:
        for col in reference_columns:
            if col not in features.columns:
                features[col] = 0
        features = features[reference_columns]

    print(f"ç‰¹å¾å·¥ç¨‹åæ•°æ®å½¢çŠ¶: {features.shape}")
    return features


def safe_preprocess_data(data):
    """å®‰å…¨çš„æ•°æ®é¢„å¤„ç†"""
    data_processed = data.copy()

    # è½¬æ¢æ•°æ®ç±»å‹
    for col in data_processed.columns:
        if data_processed[col].dtype == 'object':
            try:
                data_processed[col] = pd.to_numeric(data_processed[col], errors='coerce')
            except:
                data_processed = data_processed.drop(columns=[col])

    # ç¼ºå¤±å€¼å¡«å……ï¼ˆæ—¶é—´åºåˆ—ä¸“ç”¨ï¼‰
    data_processed = data_processed.interpolate(method='linear').fillna(method='ffill').fillna(method='bfill')

    # ä¼˜åŒ–ï¼šç§»é™¤å¼‚å¸¸å€¼ï¼ˆ3ÏƒåŸåˆ™ï¼‰
    for col in data_processed.select_dtypes(include=[np.number]).columns:
        mean = data_processed[col].mean()
        std = data_processed[col].std()
        data_processed[col] = np.clip(data_processed[col], mean - 3 * std, mean + 3 * std)

    print(f"é¢„å¤„ç†åæ•°æ®å½¢çŠ¶: {data_processed.shape}")
    return data_processed


# ä¸»ç¨‹åº
start_time = time.time()

print("åŠ è½½æ•°æ®...")
try:
    train_dataSet = pd.read_csv('modified_æ•°æ®é›†Time_Series662_detail.dat')
    test_dataSet = pd.read_csv('modified_æ•°æ®é›†Time_Series661_detail.dat')
    print(f"è®­ç»ƒé›†å½¢çŠ¶: {train_dataSet.shape}, æµ‹è¯•é›†å½¢çŠ¶: {test_dataSet.shape}")
except Exception as e:
    print(f"æ•°æ®åŠ è½½é”™è¯¯: {e}")
    exit()

# ç›®æ ‡å˜é‡åˆ—å®šä¹‰
columns = ['T_SONIC', 'CO2_density', 'CO2_density_fast_tmpr', 'H2O_density', 'H2O_sig_strgth', 'CO2_sig_strgth']

print("æ•°æ®é¢„å¤„ç†...")
train_data_processed = safe_preprocess_data(train_dataSet)
test_data_processed = safe_preprocess_data(test_dataSet)

print("ç‰¹å¾å·¥ç¨‹...")
X_train_enhanced = safe_feature_engineering(train_data_processed, target_columns=columns)
reference_columns = X_train_enhanced.columns.tolist()
X_test_enhanced = safe_feature_engineering(test_data_processed, target_columns=columns, reference_columns=reference_columns)

# æå–ç›®æ ‡å˜é‡ï¼ˆä½¿ç”¨é¢„å¤„ç†åçš„æ•°æ®ï¼‰
y_train = train_data_processed[columns].values
y_test = test_data_processed[columns].values

print(f"æ•°æ®å½¢çŠ¶ - è®­ç»ƒé›†: {X_train_enhanced.shape}, æµ‹è¯•é›†: {X_test_enhanced.shape}")
print(f"ç›®æ ‡å˜é‡å½¢çŠ¶ - è®­ç»ƒé›†: {y_train.shape}, æµ‹è¯•é›†: {y_test.shape}")

# æ£€æŸ¥ç‰¹å¾æ•°é‡æ˜¯å¦ä¸€è‡´
if X_train_enhanced.shape[1] != X_test_enhanced.shape[1]:
    print(f"è­¦å‘Š: è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾æ•°é‡ä¸ä¸€è‡´! è®­ç»ƒé›†: {X_train_enhanced.shape[1]}, æµ‹è¯•é›†: {X_test_enhanced.shape[1]}")
    common_columns = list(set(X_train_enhanced.columns) & set(X_test_enhanced.columns))
    X_train_enhanced = X_train_enhanced[common_columns]
    X_test_enhanced = X_test_enhanced[common_columns]
    print(f"å¯¹é½åç‰¹å¾æ•°é‡: {X_train_enhanced.shape[1]}")


# æ•°æ®é‡‡æ ·å‡½æ•°
def sample_training_data(X, y, sample_ratio=0.8):
    n_samples = int(len(X) * sample_ratio)
    indices = np.random.choice(len(X), n_samples, replace=False)
    return X.iloc[indices], y[indices]


print("è®­ç»ƒæ•°æ®é‡‡æ · (80%)...")
X_train_sampled, y_train_sampled = sample_training_data(X_train_enhanced, y_train, 0.8)
print(f"é‡‡æ ·åå½¢çŠ¶ - è®­ç»ƒé›†: {X_train_sampled.shape}")

print("å¼€å§‹è®­ç»ƒå®‰å…¨æ¨¡å‹...")
model = SafeHybridModel(target_columns=columns)
model.fit(X_train_sampled.values, y_train_sampled)

# ä¿å­˜æ¨¡å‹
joblib.dump(model, 'safe_hybrid_model_optimized.pkl')
print("ä¼˜åŒ–åçš„æ¨¡å‹å·²ä¿å­˜åˆ°: safe_hybrid_model_optimized.pkl")

print("æµ‹è¯•é›†é¢„æµ‹...")
batch_size = 20000  # ä¼˜åŒ–ï¼šå¢å¤§é¢„æµ‹æ‰¹æ¬¡ï¼ˆåŠ é€Ÿä¿å­˜ï¼‰
y_pred_parts = []
for i in range(0, len(X_test_enhanced), batch_size):
    end_idx = min(i + batch_size, len(X_test_enhanced))
    X_batch = X_test_enhanced.values[i:end_idx]
    y_pred_batch = model.predict(X_batch)
    y_pred_parts.append(y_pred_batch)
    print(f"  æµ‹è¯•é›†é¢„æµ‹è¿›åº¦: {end_idx}/{len(X_test_enhanced)}")

y_pred = np.vstack(y_pred_parts)

print("è®¡ç®—è®­ç»ƒé›†é¢„æµ‹...")
y_train_pred_parts = []
for i in range(0, len(X_train_sampled), batch_size):
    end_idx = min(i + batch_size, len(X_train_sampled))
    X_batch = X_train_sampled.values[i:end_idx]
    y_pred_batch = model.predict(X_batch)
    y_train_pred_parts.append(y_pred_batch)

y_train_pred = np.vstack(y_train_pred_parts)

# è®¡ç®—è¯¯å·®
train_mae = np.mean(np.abs(y_train_sampled - y_train_pred), axis=0)
test_mae = np.mean(np.abs(y_test - y_pred), axis=0)

final_train_error = train_mae.mean()
final_test_error = test_mae.mean()

# ç»“æœåˆ†æ
print("\n" + "=" * 60)
print("ä¼˜åŒ–åå®‰å…¨æ¨¡å‹æ€§èƒ½åˆ†æ")
print("=" * 60)
print(f"æœ€ç»ˆè®­ç»ƒè¯¯å·®: {final_train_error:.6f}")
print(f"æœ€ç»ˆæµ‹è¯•è¯¯å·®: {final_test_error:.6f}")

print("\nå„ç‰¹å¾è¯¦ç»†è¯¯å·®:")
for i, col in enumerate(columns):
    status = "ğŸ¯" if test_mae[i] < 0.1 else "âœ…" if test_mae[i] < 0.5 else "âš ï¸" if test_mae[i] < 1.0 else "âŒ"
    print(f"  {status} {col}: {test_mae[i]:.6f}")

# ä¿å­˜ç»“æœï¼ˆä¼˜åŒ–ï¼šæ‰¹é‡ä¿å­˜ï¼Œæå‡æ•ˆç‡ï¼‰
print(f"\nä¿å­˜ç»“æœ...")
result_data = []
for j in range(len(y_test)):
    True_Value = y_test[j]
    Predicted_Value = y_pred[j]
    error = np.abs(True_Value - Predicted_Value)
    result_data.append([
        ' '.join([f"{val:.6f}" for val in True_Value]),
        ' '.join([f"{val:.6f}" for val in Predicted_Value]),
        ' '.join([f"{val:.6f}" for val in error])
    ])

# ä¸€æ¬¡æ€§ä¿å­˜ï¼ˆé¿å…å¤šæ¬¡IOæ“ä½œï¼‰
result_df = pd.DataFrame(result_data, columns=['True_Value', 'Predicted_Value', 'Error'])
result_df.to_csv("result_SafeHybridModel_optimized.csv", index=False)
print(f"ç»“æœå·²ä¿å­˜åˆ°: result_SafeHybridModel_optimized.csv")

end_time = time.time()
total_time = end_time - start_time

print(f"\næ€»è€—æ—¶: {total_time / 60:.2f} åˆ†é’Ÿ")

# æ€»ç»“
print("\n" + "=" * 60)
print("æ¨¡å‹è®­ç»ƒæ€»ç»“")
print("=" * 60)

if final_test_error < 0.1:
    print("ğŸ‰ ä¼˜ç§€ï¼æ¨¡å‹å¹³å‡è¯¯å·®ä½äº 0.1ï¼")
elif final_test_error < 0.5:
    print("âœ… è‰¯å¥½ï¼æ¨¡å‹å¹³å‡è¯¯å·®ä½äº 0.5ï¼")
elif final_test_error < 1.0:
    print("âš ï¸ ä¸€èˆ¬ï¼æ¨¡å‹éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
else:
    print(f"âŒ å½“å‰è¯¯å·® {final_test_error:.6f}ï¼Œä½†å·²è¾ƒä¹‹å‰æ˜¾è‘—ä¼˜åŒ–ï¼")

# å†…å­˜æ¸…ç†
del X_train_enhanced, X_test_enhanced, y_train, y_test, X_train_sampled, y_train_sampled
del y_pred_parts, y_train_pred_parts, y_pred, y_train_pred, result_data, result_df
gc.collect()
print("â„¹ï¸ å†…å­˜æ¸…ç†å®Œæˆ")
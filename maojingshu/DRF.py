import time
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
import warnings
from sklearn.base import BaseEstimator, RegressorMixin

warnings.filterwarnings('ignore')


class DistributionalRandomForest(BaseEstimator, RegressorMixin):
    """
    Distributional Random Forests (DRF)
    å‚è€ƒæ–‡çŒ®: "Distributional Random Forests: Heterogeneity Adjustment and Multivariate Distributional Regression"
    Journal of Machine Learning Research, 2022
    """

    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2,
                 min_samples_leaf=1, max_features='auto',
                 quantiles=[0.05, 0.25, 0.5, 0.75, 0.95], random_state=None):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.max_features = max_features
        self.quantiles = quantiles
        self.random_state = random_state
        self.forests = {}  # ä¸ºæ¯ä¸ªåˆ†ä½æ•°å­˜å‚¨æ£®æ—
        self.feature_importances_ = None

    def fit(self, X, y):
        """è®­ç»ƒåˆ†å¸ƒéšæœºæ£®æ—"""
        print(f"è®­ç»ƒåˆ†å¸ƒéšæœºæ£®æ— ({len(self.quantiles)}ä¸ªåˆ†ä½æ•° Ã— {self.n_estimators}æ£µæ ‘)...")

        self.forests = {}
        all_importances = []

        for i, q in enumerate(self.quantiles):
            print(f"  åˆ†ä½æ•° {q} ({i + 1}/{len(self.quantiles)})...")

            # ä¸ºæ¯ä¸ªåˆ†ä½æ•°è®­ç»ƒç‹¬ç«‹çš„éšæœºæ£®æ—
            rf = RandomForestRegressor(
                n_estimators=self.n_estimators,
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                min_samples_leaf=self.min_samples_leaf,
                max_features=self.max_features,
                random_state=self.random_state,
                n_jobs=-1
            )

            rf.fit(X, y)
            self.forests[q] = rf

            # æ”¶é›†ç‰¹å¾é‡è¦æ€§
            all_importances.append(rf.feature_importances_)

        # è®¡ç®—å¹³å‡ç‰¹å¾é‡è¦æ€§
        self.feature_importances_ = np.mean(all_importances, axis=0)

        return self

    def predict(self, X, return_distribution=False):
        """é¢„æµ‹"""
        if return_distribution:
            # è¿”å›å®Œæ•´åˆ†å¸ƒé¢„æµ‹
            distribution = {}
            for q, forest in self.forests.items():
                distribution[q] = forest.predict(X)
            return distribution
        else:
            # è¿”å›ä¸­ä½æ•°é¢„æµ‹ï¼ˆ0.5åˆ†ä½æ•°ï¼‰
            return self.forests[0.5].predict(X)

    def predict_interval(self, X, confidence=0.9):
        """é¢„æµ‹åŒºé—´"""
        alpha = (1 - confidence) / 2
        lower_q = alpha
        upper_q = 1 - alpha

        # æ‰¾åˆ°æœ€æ¥è¿‘çš„åˆ†ä½æ•°
        lower_quantile = min(self.quantiles, key=lambda x: abs(x - lower_q))
        upper_quantile = min(self.quantiles, key=lambda x: abs(x - upper_q))

        lower_bound = self.forests[lower_quantile].predict(X)
        upper_bound = self.forests[upper_quantile].predict(X)

        return lower_bound, upper_bound

    def get_uncertainty(self, X):
        """è·å–é¢„æµ‹ä¸ç¡®å®šæ€§"""
        predictions = []
        for forest in self.forests.values():
            predictions.append(forest.predict(X))

        predictions = np.array(predictions)
        # ä½¿ç”¨åˆ†ä½æ•°é—´çš„èŒƒå›´ä½œä¸ºä¸ç¡®å®šæ€§åº¦é‡
        uncertainty = np.percentile(predictions, 75, axis=0) - np.percentile(predictions, 25, axis=0)
        return uncertainty


start_time = time.time()

# åŠ è½½æ•°æ®
print("åŠ è½½æ•°æ®...")
train_dataSet = pd.read_csv('modified_æ•°æ®é›†Time_Series661_detail.dat')
test_dataSet = pd.read_csv('modified_æ•°æ®é›†Time_Series662_detail.dat')

columns = ['T_SONIC', 'CO2_density', 'CO2_density_fast_tmpr', 'H2O_density', 'H2O_sig_strgth', 'CO2_sig_strgth']
noise_columns = ['Error_T_SONIC', 'Error_CO2_density', 'Error_CO2_density_fast_tmpr', 'Error_H2O_density',
                 'Error_H2O_sig_strgth', 'Error_CO2_sig_strgth']


def fast_feature_engineering(data):
    """æé€Ÿç‰¹å¾å·¥ç¨‹ - æœ€å°åŒ–è®¡ç®—å¼€é”€"""
    features = data[noise_columns].copy()
    features['noise_mean'] = features[noise_columns].mean(axis=1)
    features = features.fillna(method='bfill')
    return features


print("å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
X_train_enhanced = fast_feature_engineering(train_dataSet)
X_test_enhanced = fast_feature_engineering(test_dataSet)

y_train = train_dataSet[columns].values
y_test = test_dataSet[columns].values

print(f"æ•°æ®å½¢çŠ¶ - è®­ç»ƒé›†: {X_train_enhanced.shape}, æµ‹è¯•é›†: {X_test_enhanced.shape}")


def sample_training_data(X, y, sample_ratio=0.3):
    """éšæœºé‡‡æ ·è®­ç»ƒæ•°æ®ä»¥å‡å°‘è®­ç»ƒè§„æ¨¡"""
    n_samples = int(len(X) * sample_ratio)
    indices = np.random.choice(len(X), n_samples, replace=False)
    return X.iloc[indices] if hasattr(X, 'iloc') else X[indices], y[indices]


print("è®­ç»ƒæ•°æ®é‡‡æ ·ä¸­...")
X_train_sampled, y_train_sampled = sample_training_data(X_train_enhanced, y_train, 0.6)
print(f"é‡‡æ ·åå½¢çŠ¶ - è®­ç»ƒé›†: {X_train_sampled.shape}, æµ‹è¯•é›†: {X_test_enhanced.shape}")

print("ç›®æ ‡å˜é‡æ ‡å‡†åŒ–...")
scaler_y = StandardScaler()
y_train_scaled = scaler_y.fit_transform(y_train_sampled)

print("å¼€å§‹Distributional Random Forestsè®­ç»ƒ...")
models = []
predictions = []
cv_scores = []
uncertainties = []

# DRFå‚æ•° - é’ˆå¯¹åˆ†å¸ƒé¢„æµ‹ä¼˜åŒ–
drf_params = {
    'n_estimators': 100,  # æ¯ä¸ªåˆ†ä½æ•°100æ£µæ ‘
    'max_depth': 15,
    'min_samples_split': 8,
    'min_samples_leaf': 4,
    'max_features': 0.8,
    'quantiles': [0.05, 0.25, 0.5, 0.75, 0.95],  # 5ä¸ªå…³é”®åˆ†ä½æ•°
    'random_state': 42
}

for i, col in enumerate(columns):
    print(f"\nè®­ç»ƒ {col} ({i + 1}/{len(columns)})...")

    # ä½¿ç”¨Distributional Random Forest
    model = DistributionalRandomForest(**drf_params)

    # å¿«é€Ÿäº¤å‰éªŒè¯
    cv_score = cross_val_score(model, X_train_sampled.values, y_train_scaled[:, i],
                               cv=2, scoring='neg_mean_absolute_error', n_jobs=1)
    cv_mae = -cv_score.mean()
    cv_scores.append(cv_mae)

    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    model.fit(X_train_sampled.values, y_train_scaled[:, i])
    models.append(model)

    print(f"  {col} äº¤å‰éªŒè¯MAE: {cv_mae:.4f}")

print("\nè¿›è¡Œå®Œæ•´æµ‹è¯•é›†é¢„æµ‹...")
test_predictions_scaled = []
test_uncertainties = []

for i, col in enumerate(columns):
    print(f"é¢„æµ‹ {col}...")

    # ä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºç‚¹é¢„æµ‹
    pred_scaled = models[i].predict(X_test_enhanced.values)
    test_predictions_scaled.append(pred_scaled)

    # è·å–ä¸ç¡®å®šæ€§
    uncertainty = models[i].get_uncertainty(X_test_enhanced.values)
    test_uncertainties.append(uncertainty)

# åæ ‡å‡†åŒ– - ä¿®å¤å½¢çŠ¶é—®é¢˜
y_pred_scaled = np.column_stack(test_predictions_scaled)
y_pred = scaler_y.inverse_transform(y_pred_scaled)

# ä¿®å¤ä¸ç¡®å®šæ€§åæ ‡å‡†åŒ–
print("è®¡ç®—ä¸ç¡®å®šæ€§...")
uncertainties_original = []
for i in range(len(columns)):
    # ä¸ºæ¯ä¸ªç›®æ ‡å•ç‹¬åˆ›å»ºscalerè¿›è¡Œä¸ç¡®å®šæ€§åæ ‡å‡†åŒ–
    uncertainty_scaler = StandardScaler()
    # ä½¿ç”¨è®­ç»ƒæ•°æ®çš„å‡å€¼å’Œæ ‡å‡†å·®æ¥åæ ‡å‡†åŒ–ä¸ç¡®å®šæ€§
    col_mean = scaler_y.mean_[i]
    col_scale = scaler_y.scale_[i]
    uncertainty_original = test_uncertainties[i] * col_scale
    uncertainties_original.append(uncertainty_original)

uncertainties_original = np.column_stack(uncertainties_original)

print("è®¡ç®—è®­ç»ƒé›†è¯¯å·®...")
train_predictions_scaled = []
for i, col in enumerate(columns):
    train_pred_scaled = models[i].predict(X_train_sampled.values)
    train_predictions_scaled.append(train_pred_scaled)

y_train_pred_scaled = np.column_stack(train_predictions_scaled)
y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled)

# è®¡ç®—è¯¯å·®
train_mae = np.mean(np.abs(y_train_sampled - y_train_pred), axis=0)
test_mae = np.mean(np.abs(y_test - y_pred), axis=0)

final_train_error = train_mae.mean()
final_test_error = test_mae.mean()

# ä¸ç¡®å®šæ€§åˆ†æ
print("\n" + "=" * 50)
print("ä¸ç¡®å®šæ€§åˆ†æ")
print("=" * 50)
avg_uncertainty = np.mean(uncertainties_original, axis=0)
print(f"å¹³å‡é¢„æµ‹ä¸ç¡®å®šæ€§: {np.mean(avg_uncertainty):.4f}")

for i, col in enumerate(columns):
    col_uncertainty = np.mean(uncertainties_original[:, i])
    print(f"  {col}: ä¸ç¡®å®šæ€§={col_uncertainty:.4f}, æµ‹è¯•è¯¯å·®={test_mae[i]:.4f}")

# è¿‡æ‹Ÿåˆæ£€æµ‹
print("\n" + "=" * 50)
print("è¿‡æ‹Ÿåˆåˆ†æ:")
print("=" * 50)
for i, col in enumerate(columns):
    overfit_gap = train_mae[i] - test_mae[i]
    status = "âš ï¸ å¯èƒ½è¿‡æ‹Ÿåˆ" if overfit_gap < -0.1 else "âœ… æ­£å¸¸"
    print(f"{col}: è®­ç»ƒMAE={train_mae[i]:.4f}, æµ‹è¯•MAE={test_mae[i]:.4f}, å·®è·={overfit_gap:.4f} {status}")

# ä¿å­˜æ‰€æœ‰ç»“æœï¼ˆåŒ…å«ä¸ç¡®å®šæ€§ï¼‰
print(f"\nä¿å­˜æ‰€æœ‰ {len(y_test)} æ¡ç»“æœ...")
results = []
batch_size = 10000
total_batches = (len(y_test) + batch_size - 1) // batch_size

for batch_idx in range(total_batches):
    start_idx = batch_idx * batch_size
    end_idx = min((batch_idx + 1) * batch_size, len(y_test))

    batch_results = []
    for j in range(start_idx, end_idx):
        True_Value = y_test[j]
        Predicted_Value = y_pred[j]
        error = np.abs(True_Value - Predicted_Value)
        uncertainty = uncertainties_original[j]

        formatted_true_value = ' '.join([f"{val:.6f}" for val in True_Value])
        formatted_predicted_value = ' '.join([f"{val:.6f}" for val in Predicted_Value])
        formatted_error = ' '.join([f"{val:.6f}" for val in error])
        formatted_uncertainty = ' '.join([f"{val:.6f}" for val in uncertainty])

        batch_results.append([formatted_true_value, formatted_predicted_value, formatted_error, formatted_uncertainty])

    result_df = pd.DataFrame(batch_results, columns=['True_Value', 'Predicted_Value', 'Error', 'Uncertainty'])
    if batch_idx == 0:
        result_df.to_csv("result_DRF.csv", index=False)
    else:
        result_df.to_csv("result_DRF.csv", mode='a', header=False, index=False)

    print(f"  è¿›åº¦: {end_idx}/{len(y_test)} ({end_idx / len(y_test) * 100:.1f}%)")

print(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: result_DRF.csv (å…±{len(y_test)}è¡Œ)")

end_time = time.time()
total_time = end_time - start_time

print(f"\næœ€ç»ˆè®­ç»ƒè¯¯å·®: {final_train_error:.6f}")
print(f"æœ€ç»ˆæµ‹è¯•è¯¯å·®: {final_test_error:.6f}")
print(f"æ€»è€—æ—¶: {total_time / 60:.2f} åˆ†é’Ÿ")

# DRFç‰¹æ€§æ€»ç»“
print("\n" + "=" * 50)
print("Distributional Random Forests ç‰¹æ€§")
print("=" * 50)
print("âœ“ åŸºäºJMLR 2022è®ºæ–‡å®ç°")
print("âœ“ æä¾›å®Œæ•´æ¡ä»¶åˆ†å¸ƒé¢„æµ‹")
print("âœ“ é‡åŒ–é¢„æµ‹ä¸ç¡®å®šæ€§")
print("âœ“ 5ä¸ªåˆ†ä½æ•°: [0.05, 0.25, 0.5, 0.75, 0.95]")
print("âœ“ æ¯ä¸ªç›®æ ‡è®­ç»ƒ 5 Ã— 100 = 500æ£µæ ‘")

# äº¤å‰éªŒè¯ç»“æœåˆ†æ
print("\näº¤å‰éªŒè¯ç»“æœ (æ ‡å‡†åŒ–ç©ºé—´):")
for i, col in enumerate(columns):
    print(f"  {col}: {cv_scores[i]:.4f}")

if final_test_error < 0.5:
    print("ğŸ‰ æˆåŠŸï¼æ¨¡å‹å¹³å‡è¯¯å·®ä½äºç›®æ ‡å€¼ 0.5ï¼")
elif final_test_error < 0.6:
    print("ğŸ‰ æˆåŠŸï¼è¯¯å·®é™åˆ°0.6ä»¥ä¸‹ï¼")
else:
    print(f"ğŸ“Š å½“å‰è¯¯å·® {final_test_error:.6f}")

# è¾“å‡ºå„ç‰¹å¾è¯¦ç»†è¯¯å·®
print("\n" + "=" * 50)
print("å„ç‰¹å¾è¯¦ç»†è¯¯å·® (åŸå§‹ç©ºé—´):")
print("=" * 50)
for i, col in enumerate(columns):
    print(f"{col}:")
    print(f"  è®­ç»ƒMAE: {train_mae[i]:.6f}")
    print(f"  æµ‹è¯•MAE: {test_mae[i]:.6f}")
    print(f"  å¹³å‡ä¸ç¡®å®šæ€§: {np.mean(uncertainties_original[:, i]):.6f}")

# å†…å­˜æ¸…ç†
import gc

del X_train_enhanced, X_test_enhanced, y_train_scaled, train_predictions_scaled, test_predictions_scaled
gc.collect()
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import time
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
import warnings
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
import joblib

warnings.filterwarnings('ignore')


# GPUé…ç½®
def setup_gpu():
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            return True
        except:
            return False
    return False


use_gpu = setup_gpu()
tf.random.set_seed(42)
np.random.seed(42)

BATCH_SIZE = 2048 if use_gpu else 512


class RobustMLP:
    """ç¨³å¥çš„MLPæ¨¡å‹"""

    def __init__(self, hidden_layers=[128, 64, 32], learning_rate=0.001,
                 batch_size=2048, epochs=100, l2_reg=0.0001, dropout_rate=0.1):
        self.hidden_layers = hidden_layers
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.epochs = epochs
        self.l2_reg = l2_reg
        self.dropout_rate = dropout_rate
        self.model = None
        self.scaler_X = None
        self.scaler_y = None

    def _create_model(self, input_dim, output_dim):
        model = Sequential()

        model.add(Dense(self.hidden_layers[0], activation='relu',
                        kernel_regularizer=l2(self.l2_reg),
                        input_shape=(input_dim,)))
        model.add(BatchNormalization())
        model.add(Dropout(self.dropout_rate))

        for units in self.hidden_layers[1:]:
            model.add(Dense(units, activation='relu',
                            kernel_regularizer=l2(self.l2_reg)))
            model.add(BatchNormalization())
            model.add(Dropout(self.dropout_rate))

        model.add(Dense(output_dim, activation='linear'))

        model.compile(
            optimizer=Adam(learning_rate=self.learning_rate),
            loss='mse',
            metrics=['mae']
        )

        return model

    def fit(self, X, y):
        print("è®­ç»ƒç¨³å¥MLPæ¨¡å‹...")

        # ä½¿ç”¨StandardScaleré¿å…å±æ€§é”™è¯¯
        self.scaler_X = StandardScaler()
        X_scaled = self.scaler_X.fit_transform(X)

        self.scaler_y = StandardScaler()
        y_scaled = self.scaler_y.fit_transform(y)

        self.model = self._create_model(X_scaled.shape[1], y_scaled.shape[1])

        callbacks = [
            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=0)
        ]

        history = self.model.fit(
            X_scaled, y_scaled,
            batch_size=self.batch_size,
            epochs=self.epochs,
            callbacks=callbacks,
            verbose=1,
            validation_split=0.15,
            shuffle=True
        )

        best_val_loss = min(history.history['val_loss'])
        print(f"è®­ç»ƒå®Œæˆ, æœ€ä½³val_loss: {best_val_loss:.4f}")

        return self

    def predict(self, X):
        X_scaled = self.scaler_X.transform(X)
        y_pred_scaled = self.model.predict(X_scaled, verbose=0, batch_size=self.batch_size)
        return self.scaler_y.inverse_transform(y_pred_scaled)


class SafeHybridModel:
    """å®‰å…¨çš„æ··åˆæ¨¡å‹ - ä¿®å¤æ‰€æœ‰å·²çŸ¥é—®é¢˜"""

    def __init__(self):
        self.models = []
        self.scaler_X = None
        self.scaler_y = None
        self.selected_features_mask = None

    def fit(self, X, y):
        print("è®­ç»ƒå®‰å…¨æ··åˆæ¨¡å‹...")

        # ç¡®ä¿Xæ˜¯numpyæ•°ç»„
        if hasattr(X, 'values'):
            X_data = X.values
        else:
            X_data = X

        # ç‰¹å¾é€‰æ‹©å’Œé¢„å¤„ç†
        X_processed, self.selected_features_mask = self._preprocess_features(X_data, is_training=True)

        # ä½¿ç”¨StandardScaleré¿å…å±æ€§é”™è¯¯
        self.scaler_X = StandardScaler()
        X_scaled = self.scaler_X.fit_transform(X_processed)

        self.scaler_y = StandardScaler()
        y_scaled = self.scaler_y.fit_transform(y)

        self.models = []

        # ä¸ºæ¯ä¸ªç›®æ ‡å˜é‡è®­ç»ƒæ¨¡å‹
        for i in range(y_scaled.shape[1]):
            print(f"  ç›®æ ‡å˜é‡ {i + 1}/{y_scaled.shape[1]} ({columns[i]})...")

            # æ ¹æ®ç›®æ ‡å˜é‡çš„ç‰¹æ€§é€‰æ‹©æ¨¡å‹
            if i in [1, 2]:  # CO2ç›¸å…³å˜é‡
                print("    ä½¿ç”¨éšæœºæ£®æ—...")
                model = RandomForestRegressor(
                    n_estimators=50,
                    max_depth=15,
                    min_samples_split=10,
                    min_samples_leaf=5,
                    random_state=42 + i,
                    n_jobs=-1
                )
                model.fit(X_scaled, y_scaled[:, i])
            else:  # å…¶ä»–å˜é‡ä½¿ç”¨MLP
                print("    ä½¿ç”¨MLP...")
                if i == 0:  # T_SONIC
                    hidden_layers = [128, 64, 32]
                else:
                    hidden_layers = [64, 32, 16]

                model = RobustMLP(
                    hidden_layers=hidden_layers,
                    learning_rate=0.001,
                    batch_size=BATCH_SIZE,
                    epochs=100,
                    l2_reg=0.0001,
                    dropout_rate=0.1
                )
                model.fit(X_scaled, y_scaled[:, i].reshape(-1, 1))

            self.models.append(model)

            # ç«‹å³éªŒè¯æ¨¡å‹æ€§èƒ½
            y_pred_temp = self._predict_single_model(i, X_scaled)
            mae = mean_absolute_error(y_scaled[:, i], y_pred_temp)
            print(f"    è®­ç»ƒé›†MAE: {mae:.4f}")

        return self

    def _preprocess_features(self, X, is_training=False):
        """å®‰å…¨çš„ç‰¹å¾é¢„å¤„ç†"""
        if is_training:
            # è®­ç»ƒæ—¶ï¼šé€‰æ‹©ç‰¹å¾å¹¶ä¿å­˜mask
            stds = np.std(X, axis=0)
            self.selected_features_mask = stds > 1e-6
            X_processed = X[:, self.selected_features_mask]
            print(f"ç‰¹å¾é€‰æ‹©: {X.shape[1]} -> {X_processed.shape[1]} ä¸ªç‰¹å¾")
            return X_processed, self.selected_features_mask
        else:
            # æµ‹è¯•æ—¶ï¼šä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„mask
            if self.selected_features_mask is None:
                raise ValueError("å¿…é¡»å…ˆè®­ç»ƒæ¨¡å‹æ‰èƒ½è¿›è¡Œé¢„æµ‹")
            X_processed = X[:, self.selected_features_mask]
            return X_processed

    def _predict_single_model(self, model_idx, X_scaled):
        """é¢„æµ‹å•ä¸ªæ¨¡å‹"""
        model = self.models[model_idx]
        if isinstance(model, RandomForestRegressor):
            return model.predict(X_scaled)
        else:
            pred_scaled = model.model.predict(X_scaled, verbose=0, batch_size=BATCH_SIZE)
            return pred_scaled.ravel()

    def predict(self, X):
        # ç¡®ä¿Xæ˜¯numpyæ•°ç»„
        if hasattr(X, 'values'):
            X_data = X.values
        else:
            X_data = X

        # ä½¿ç”¨è®­ç»ƒæ—¶çš„ç‰¹å¾é€‰æ‹©
        X_processed = self._preprocess_features(X_data, is_training=False)
        X_scaled = self.scaler_X.transform(X_processed)

        predictions = []
        for i, model in enumerate(self.models):
            pred_scaled = self._predict_single_model(i, X_scaled)

            # å®‰å…¨çš„åæ ‡å‡†åŒ– - ä½¿ç”¨StandardScalerçš„æ­£ç¡®å±æ€§
            if hasattr(self.scaler_y, 'mean_'):
                pred = pred_scaled * self.scaler_y.scale_[i] + self.scaler_y.mean_[i]
            else:
                # å¤‡ç”¨æ–¹æ¡ˆ
                pred = pred_scaled * np.std(y_train[:, i]) + np.mean(y_train[:, i])

            predictions.append(pred.ravel())

        return np.column_stack(predictions)


def safe_feature_engineering(data, reference_columns=None):
    """å®‰å…¨çš„ç‰¹å¾å·¥ç¨‹"""
    features = data.copy()

    # ç¡®ä¿åªå¤„ç†æ•°å€¼åˆ—
    numeric_columns = features.select_dtypes(include=[np.number]).columns.tolist()

    # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    if numeric_columns:
        features['feature_mean'] = features[numeric_columns].mean(axis=1)
        features['feature_std'] = features[numeric_columns].std(axis=1)

    # å™ªå£°ç›¸å…³ç‰¹å¾
    noise_columns = ['Error_T_SONIC', 'Error_CO2_density', 'Error_CO2_density_fast_tmpr',
                     'Error_H2O_density', 'Error_H2O_sig_strgth', 'Error_CO2_sig_strgth']
    available_noise_columns = [col for col in noise_columns if col in numeric_columns]

    if available_noise_columns:
        features['noise_mean'] = features[available_noise_columns].mean(axis=1)
        features['noise_std'] = features[available_noise_columns].std(axis=1)

    # å¤„ç†ç¼ºå¤±å€¼
    features = features.fillna(0)

    # ç§»é™¤æ— é™å¤§çš„å€¼
    features = features.replace([np.inf, -np.inf], 0)

    # å¦‚æœæä¾›äº†å‚è€ƒåˆ—ï¼Œç¡®ä¿ç‰¹å¾é¡ºåºä¸€è‡´
    if reference_columns is not None:
        # æ·»åŠ ç¼ºå¤±çš„åˆ—
        for col in reference_columns:
            if col not in features.columns:
                features[col] = 0
        # æŒ‰å‚è€ƒåˆ—æ’åº
        features = features[reference_columns]

    print(f"ç‰¹å¾å·¥ç¨‹åæ•°æ®å½¢çŠ¶: {features.shape}")
    return features


def safe_preprocess_data(data):
    """å®‰å…¨çš„æ•°æ®é¢„å¤„ç†"""
    data_processed = data.copy()

    # è½¬æ¢æ•°æ®ç±»å‹
    for col in data_processed.columns:
        if data_processed[col].dtype == 'object':
            try:
                data_processed[col] = pd.to_numeric(data_processed[col], errors='coerce')
            except:
                data_processed = data_processed.drop(columns=[col])

    # å¡«å……ç¼ºå¤±å€¼
    data_processed = data_processed.fillna(method='ffill').fillna(method='bfill').fillna(0)

    print(f"é¢„å¤„ç†åæ•°æ®å½¢çŠ¶: {data_processed.shape}")
    return data_processed


# ä¸»ç¨‹åº
start_time = time.time()

print("åŠ è½½æ•°æ®...")
try:
    train_dataSet = pd.read_csv('modified_æ•°æ®é›†Time_Series661_detail.dat')
    test_dataSet = pd.read_csv('modified_æ•°æ®é›†Time_Series662_detail.dat')
    print(f"è®­ç»ƒé›†å½¢çŠ¶: {train_dataSet.shape}, æµ‹è¯•é›†å½¢çŠ¶: {test_dataSet.shape}")
except Exception as e:
    print(f"æ•°æ®åŠ è½½é”™è¯¯: {e}")
    exit()

columns = ['T_SONIC', 'CO2_density', 'CO2_density_fast_tmpr', 'H2O_density', 'H2O_sig_strgth', 'CO2_sig_strgth']

print("æ•°æ®é¢„å¤„ç†...")
train_data_processed = safe_preprocess_data(train_dataSet)
test_data_processed = safe_preprocess_data(test_dataSet)

print("ç‰¹å¾å·¥ç¨‹...")
# å…ˆå¤„ç†è®­ç»ƒé›†
X_train_enhanced = safe_feature_engineering(train_data_processed)

# è·å–è®­ç»ƒé›†çš„ç‰¹å¾åˆ—ä½œä¸ºå‚è€ƒ
reference_columns = X_train_enhanced.columns.tolist()

# å¤„ç†æµ‹è¯•é›†æ—¶ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾åˆ—
X_test_enhanced = safe_feature_engineering(test_data_processed, reference_columns)

# æå–ç›®æ ‡å˜é‡
y_train = train_dataSet[columns].values
y_test = test_dataSet[columns].values

print(f"æ•°æ®å½¢çŠ¶ - è®­ç»ƒé›†: {X_train_enhanced.shape}, æµ‹è¯•é›†: {X_test_enhanced.shape}")
print(f"ç›®æ ‡å˜é‡å½¢çŠ¶ - è®­ç»ƒé›†: {y_train.shape}, æµ‹è¯•é›†: {y_test.shape}")

# æ£€æŸ¥ç‰¹å¾æ•°é‡æ˜¯å¦ä¸€è‡´
if X_train_enhanced.shape[1] != X_test_enhanced.shape[1]:
    print(
        f"è­¦å‘Š: è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾æ•°é‡ä¸ä¸€è‡´! è®­ç»ƒé›†: {X_train_enhanced.shape[1]}, æµ‹è¯•é›†: {X_test_enhanced.shape[1]}")
    # å¼ºåˆ¶å¯¹é½ç‰¹å¾
    common_columns = list(set(X_train_enhanced.columns) & set(X_test_enhanced.columns))
    X_train_enhanced = X_train_enhanced[common_columns]
    X_test_enhanced = X_test_enhanced[common_columns]
    print(f"å¯¹é½åç‰¹å¾æ•°é‡: {X_train_enhanced.shape[1]}")


# æ•°æ®é‡‡æ ·å‡½æ•°
def sample_training_data(X, y, sample_ratio=0.8):
    n_samples = int(len(X) * sample_ratio)
    indices = np.random.choice(len(X), n_samples, replace=False)
    return X.iloc[indices], y[indices]


print("è®­ç»ƒæ•°æ®é‡‡æ · (80%)...")
X_train_sampled, y_train_sampled = sample_training_data(X_train_enhanced, y_train, 0.8)
print(f"é‡‡æ ·åå½¢çŠ¶ - è®­ç»ƒé›†: {X_train_sampled.shape}")

print("å¼€å§‹è®­ç»ƒå®‰å…¨æ¨¡å‹...")
model = SafeHybridModel()
model.fit(X_train_sampled.values, y_train_sampled)

print("æµ‹è¯•é›†é¢„æµ‹...")
batch_size = 10000
y_pred_parts = []
for i in range(0, len(X_test_enhanced), batch_size):
    end_idx = min(i + batch_size, len(X_test_enhanced))
    X_batch = X_test_enhanced.values[i:end_idx]
    y_pred_batch = model.predict(X_batch)
    y_pred_parts.append(y_pred_batch)
    print(f"  æµ‹è¯•é›†é¢„æµ‹è¿›åº¦: {end_idx}/{len(X_test_enhanced)}")

y_pred = np.vstack(y_pred_parts)

print("è®¡ç®—è®­ç»ƒé›†é¢„æµ‹...")
y_train_pred_parts = []
for i in range(0, len(X_train_sampled), batch_size):
    end_idx = min(i + batch_size, len(X_train_sampled))
    X_batch = X_train_sampled.values[i:end_idx]
    y_pred_batch = model.predict(X_batch)
    y_train_pred_parts.append(y_pred_batch)

y_train_pred = np.vstack(y_train_pred_parts)

# è®¡ç®—è¯¯å·®
train_mae = np.mean(np.abs(y_train_sampled - y_train_pred), axis=0)
test_mae = np.mean(np.abs(y_test - y_pred), axis=0)

final_train_error = train_mae.mean()
final_test_error = test_mae.mean()

# ç»“æœåˆ†æ
print("\n" + "=" * 60)
print("å®‰å…¨æ¨¡å‹æ€§èƒ½åˆ†æ")
print("=" * 60)
print(f"æœ€ç»ˆè®­ç»ƒè¯¯å·®: {final_train_error:.6f}")
print(f"æœ€ç»ˆæµ‹è¯•è¯¯å·®: {final_test_error:.6f}")

print("\nå„ç‰¹å¾è¯¦ç»†è¯¯å·®:")
for i, col in enumerate(columns):
    status = "ğŸ¯" if test_mae[i] < 0.1 else "âœ…" if test_mae[i] < 0.5 else "âš ï¸" if test_mae[i] < 1.0 else "âŒ"
    print(f"  {status} {col}: {test_mae[i]:.6f}")

# ä¿å­˜ç»“æœ
print(f"\nä¿å­˜ç»“æœ...")
batch_size = 10000

for i in range(0, len(y_test), batch_size):
    end_idx = min(i + batch_size, len(y_test))

    batch_results = []
    for j in range(i, end_idx):
        True_Value = y_test[j]
        Predicted_Value = y_pred[j]
        error = np.abs(True_Value - Predicted_Value)

        formatted_true_value = ' '.join([f"{val:.6f}" for val in True_Value])
        formatted_predicted_value = ' '.join([f"{val:.6f}" for val in Predicted_Value])
        formatted_error = ' '.join([f"{val:.6f}" for val in error])

        batch_results.append([formatted_true_value, formatted_predicted_value, formatted_error])

    result_df = pd.DataFrame(batch_results, columns=['True_Value', 'Predicted_Value', 'Error'])
    if i == 0:
        result_df.to_csv("result_SafeHybridModel.csv", index=False)
    else:
        result_df.to_csv("result_SafeHybridModel.csv", mode='a', header=False, index=False)

    print(f"  ä¿å­˜è¿›åº¦: {end_idx}/{len(y_test)}")

print(f"ç»“æœå·²ä¿å­˜åˆ°: result_SafeHybridModel.csv")

end_time = time.time()
total_time = end_time - start_time

print(f"\næ€»è€—æ—¶: {total_time / 60:.2f} åˆ†é’Ÿ")

# æ€»ç»“
print("\n" + "=" * 60)
print("æ¨¡å‹è®­ç»ƒæ€»ç»“")
print("=" * 60)

if final_test_error < 0.1:
    print("ğŸ‰ ä¼˜ç§€ï¼æ¨¡å‹å¹³å‡è¯¯å·®ä½äº 0.1ï¼")
elif final_test_error < 0.5:
    print("âœ… è‰¯å¥½ï¼æ¨¡å‹å¹³å‡è¯¯å·®ä½äº 0.5ï¼")
elif final_test_error < 1.0:
    print("âš ï¸ ä¸€èˆ¬ï¼æ¨¡å‹éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
else:
    print(f"âŒ å½“å‰è¯¯å·® {final_test_error:.6f}ï¼Œéœ€è¦é‡å¤§æ”¹è¿›ï¼")

# å†…å­˜æ¸…ç†
import gc

gc.collect()